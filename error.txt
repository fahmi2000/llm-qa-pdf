C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\venv\Lib\site-packages\huggingface_hub\utils\_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.
  warnings.warn(warning_message, FutureWarning)
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Number of tokens in input: 2
2023-11-17 11:43:49.790 Uncaught app exception
Traceback (most recent call last):
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\venv\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py", line 534, in _run_script
    exec(code, module.__dict__)
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\app.py", line 127, in <module>
    main()
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\app.py", line 104, in main
    handle_userinput(user_question)
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\app.py", line 81, in handle_userinput
    response = st.session_state.conversation({'question': user_question})
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\venv\Lib\site-packages\langchain\chains\base.py", 
line 310, in __call__
    raise e
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\venv\Lib\site-packages\langchain\chains\base.py", 
line 304, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\venv\Lib\site-packages\langchain\chains\conversational_retrieval\base.py", line 159, in _call
    answer = self.combine_docs_chain.run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\venv\Lib\site-packages\langchain\chains\base.py", 
line 510, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\venv\Lib\site-packages\langchain\chains\base.py", 
line 310, in __call__
    raise e
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\venv\Lib\site-packages\langchain\chains\base.py", 
line 304, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\venv\Lib\site-packages\langchain\chains\combine_documents\base.py", line 122, in _call
    output, extra_return_dict = self.combine_docs(
                                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\venv\Lib\site-packages\langchain\chains\combine_documents\stuff.py", line 171, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\venv\Lib\site-packages\langchain\chains\llm.py", line 298, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\venv\Lib\site-packages\langchain\chains\base.py", 
line 310, in __call__
    raise e
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\venv\Lib\site-packages\langchain\chains\base.py", 
line 304, in __call__
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\venv\Lib\site-packages\langchain\chains\llm.py", line 108, in _call
    response = self.generate([inputs], run_manager=run_manager)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\venv\Lib\site-packages\langchain\chains\llm.py", line 120, in generate
    return self.llm.generate_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\venv\Lib\site-packages\langchain\llms\base.py", line 507, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\venv\Lib\site-packages\langchain\llms\base.py", line 656, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\venv\Lib\site-packages\langchain\llms\base.py", line 544, in _generate_helper
    raise e
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\venv\Lib\site-packages\langchain\llms\base.py", line 531, in _generate_helper
    self._generate(
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\venv\Lib\site-packages\langchain\llms\base.py", line 1053, in _generate
    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)
  File "C:\Users\Aliff Fahmi\Documents\GitHub\Local-LLM-and-OpenAI-PDF-QA\venv\Lib\site-packages\langchain\llms\huggingface_hub.py", line 112, in _call
    raise ValueError(f"Error raised by inference API: {response['error']}")
ValueError: Error raised by inference API: Input validation error: `inputs` must have less than 1024 tokens. Given: 1181  